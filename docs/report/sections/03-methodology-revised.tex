\section{Methodology}
\label{sec:methodology}

\subsection{Analysis Framework}

This dependability analysis follows a systematic, criterion-based evaluation approach. Nine distinct criteria address different quality dimensions, each employing specific tools and producing quantifiable metrics. The criteria interconnect to provide a holistic assessment: static analysis identifies potential issues, coverage metrics reveal testing gaps, mutation testing validates test effectiveness, and security scanning ensures vulnerability awareness.

\subsection{Experimental Environment}

Analysis was conducted across two environments to ensure reproducibility and validate CI/CD integration. Local development used macOS Sonoma with Java 21 (Eclipse Temurin distribution), while automated pipelines executed on Ubuntu 22.04 runners in GitHub Actions. Maven 3.9.x, accessed through the Maven Wrapper to ensure version consistency, served as the build tool. Docker Desktop provided local container runtime, with equivalent functionality in GitHub Actions for automated builds.

Table~\ref{tab:environment} summarizes the experimental configuration.

\begin{table}[h]
\centering
\caption{Experimental Environment Specifications}
\label{tab:environment}
\begin{tabular}{ll}
\toprule
\textbf{Component} & \textbf{Specification} \\
\midrule
Operating System (Local) & macOS Sonoma \\
Operating System (CI) & Ubuntu 22.04 \\
Java Distribution & Eclipse Temurin OpenJDK 21 \\
Build Tool & Maven 3.9.x (Maven Wrapper) \\
CI/CD Platform & GitHub Actions \\
Container Runtime & Docker Desktop / GitHub Actions \\
Code Analysis & SonarCloud \\
\bottomrule
\end{tabular}
\end{table}

The analysis was performed on a fork of the official Spring PetClinic repository, with all modifications, configurations, and results tracked in version control. The project repository, SonarCloud dashboard, and DockerHub image are publicly accessible for verification and reproducibility.

\subsection{Evaluation Criteria}

\subsubsection{Criterion 1: CI/CD Pipeline}

The first criterion establishes automated build and quality assurance infrastructure. GitHub Actions workflows were configured to execute on push events to the main branch, pull requests, and manual dispatch. The pipeline encompasses compilation, unit test execution, integration test execution, coverage reporting, and artifact publication. Success is measured by consistent build success rate (target: 100\%), reasonable build duration (target: under 5 minutes), and complete test execution without failures.

\subsubsection{Criterion 2: Static Code Analysis}

Static analysis via SonarCloud identifies quality issues without executing the code. The analysis categorizes findings as bugs (potential runtime errors), vulnerabilities (security weaknesses), security hotspots (security-sensitive code requiring review), and code smells (maintainability issues). Issues are prioritized by severity from blocker through critical, major, minor, to informational. Target metrics include zero bugs, zero vulnerabilities, and maintainability rating of A.

\subsubsection{Criteria 3-4: Docker Containerization}

Containerization criteria address both image creation (Criterion 3) and container execution (Criterion 4). A multi-stage Dockerfile separates build-time dependencies from the runtime image, reducing size and attack surface. Security best practices include running as a non-root user and configuring health checks for orchestration compatibility. Success requires successful image build, publication to DockerHub, and verified container execution with accessible application endpoints.

\subsubsection{Criterion 5: Test Coverage}

JaCoCo measures code coverage across multiple dimensions. The Maven plugin instruments classes during test execution and generates HTML and XML reports. Coverage is analyzed by package and class to identify areas requiring additional testing. The target threshold of 80\% line coverage represents industry best practice, with branch coverage providing additional insight into decision point testing.

\subsubsection{Criterion 6: Mutation Testing}

PITest mutation testing evaluates test effectiveness beyond coverage metrics. The mutation score is calculated according to Equation~\ref{eq:mutation-score}.

\begin{equation}
\label{eq:mutation-score}
\text{Mutation Score} = \frac{\text{Killed Mutants}}{\text{Total Mutants} - \text{Equivalent Mutants}} \times 100\%
\end{equation}

Configuration targets the main application packages with standard mutators. Surviving mutants are analyzed to identify test improvement opportunities. A mutation score above 75\% indicates effective testing.

\subsubsection{Criterion 7: Performance Benchmarking}

JMH benchmarks measure execution time for critical repository operations. Benchmark configuration includes warmup iterations (3 iterations, 1 second each) to stabilize JIT compilation, measurement iterations (5 iterations, 1 second each) for data collection, and execution in AverageTime mode reporting milliseconds per operation. The H2 in-memory database profile ensures isolation from external database performance characteristics.

\subsubsection{Criterion 8: Automated Test Generation}

Randoop generates regression tests through feedback-directed random testing. Generation parameters include time budget per class, target packages, and output format. Generated tests are evaluated for coverage contribution, assertion quality, and integration with existing test infrastructure. JUnit 4 output requires migration to JUnit 5 for consistency with the project test suite.

\subsubsection{Criterion 9: Security Analysis}

Security analysis combines SpotBugs with the FindSecBugs plugin for static security analysis and OWASP Dependency-Check for dependency vulnerability scanning. Findings are categorized by severity and mapped to security risk taxonomies. The target is zero critical or high severity vulnerabilities, with documented assessment of any medium or low severity findings.

\subsection{Data Collection and Analysis}

Each criterion produces structured output amenable to quantitative analysis. Coverage reports provide line, branch, and method percentages. Mutation testing generates detailed mutant status and location data. Benchmark results include statistical measures with error margins. All raw data is preserved in the repository for transparency and reproducibility.

Analysis follows a systematic process: execute tools with documented configuration, collect and validate output, extract relevant metrics, compare against targets, and document findings with supporting evidence. Challenges and anomalies are investigated, with solutions or workarounds documented for future reference.

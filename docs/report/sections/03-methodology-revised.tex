\section{Methodology}
\label{sec:methodology}

This section describes the systematic approach adopted for conducting the dependability analysis of the Spring PetClinic application. The methodology encompasses the definition of baseline state, experimental setup, tool selection, and evaluation criteria aligned with academic requirements.

\subsection{Baseline State Definition}

Before implementing any enhancements, we documented the original state of the Spring PetClinic project to enable meaningful before/after comparisons. Table~\ref{tab:baseline-state} summarizes the initial conditions across all relevant dimensions.

\begin{table}[htbp]
\centering
\caption{Baseline State of Spring PetClinic Before Analysis}
\label{tab:baseline-state}
\begin{tabular}{lll}
\toprule
\textbf{Dimension} & \textbf{Initial State} & \textbf{Notes} \\
\midrule
CI/CD Pipeline & 1 workflow (basic build) & build.yml only \\
Code Coverage Tool & Not integrated & No JaCoCo \\
Coverage Measurement & Not measured & Unknown baseline \\
Mutation Testing & Not configured & No PITest \\
Static Analysis & Not integrated & No SonarCloud \\
Docker Configuration & Basic Dockerfile & No health check \\
Performance Benchmarks & None & No JMH \\
Automated Test Generation & None & No Randoop \\
Security Scanning & None & No OWASP/SpotBugs \\
Test Suite Size & 39 tests & Original tests only \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Experimental Environment}

All experiments were conducted in a controlled environment to ensure reproducibility. Table~\ref{tab:environment} details the technical specifications.

\begin{table}[htbp]
\centering
\caption{Experimental Environment Specifications}
\label{tab:environment}
\begin{tabular}{ll}
\toprule
\textbf{Component} & \textbf{Specification} \\
\midrule
Operating System & macOS (local), Ubuntu 22.04 (CI) \\
Java Version & Eclipse Temurin 21 \\
Build Tool & Apache Maven 3.9.x \\
Spring Boot Version & 4.0.0-SNAPSHOT (M3) \\
Container Runtime & Docker 24.x \\
CI/CD Platform & GitHub Actions \\
Code Analysis Platform & SonarCloud \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Tool Selection and Justification}

Each tool was selected based on specific criteria including industry adoption, academic relevance, and integration capabilities. Table~\ref{tab:tools} presents the complete toolchain.

\begin{table}[htbp]
\centering
\caption{Analysis Tools and Selection Rationale}
\label{tab:tools}
\begin{tabular}{lll}
\toprule
\textbf{Tool} & \textbf{Purpose} & \textbf{Rationale} \\
\midrule
JaCoCo 0.8.12 & Code coverage & Industry standard, Maven integration \\
PITest 1.15.0 & Mutation testing & Leading Java mutation tool \\
SonarCloud & Static analysis & Free for open source, comprehensive \\
JMH 1.37 & Performance & OpenJDK official benchmark framework \\
Randoop 4.3.2 & Test generation & Academic and industry proven \\
OWASP DC 11.1.1 & Dependency security & NIST NVD integration \\
SpotBugs 4.8.6 & Security patterns & FindSecBugs plugin \\
Docker & Containerization & Industry standard \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Evaluation Criteria}

The project addresses nine specific criteria established by the course requirements. Table~\ref{tab:criteria} maps each criterion to its verification method.

\begin{table}[htbp]
\centering
\caption{Nine Evaluation Criteria and Verification Methods}
\label{tab:criteria}
\begin{tabular}{clll}
\toprule
\textbf{\#} & \textbf{Criterion} & \textbf{Tool/Method} & \textbf{Success Metric} \\
\midrule
1 & CI/CD Pipeline & GitHub Actions & Green build status \\
2 & SonarCloud Analysis & SonarCloud & AAA rating \\
3 & Docker Image & DockerHub & Successful push \\
4 & Container Health & Docker & Health check passing \\
5 & Code Coverage & JaCoCo & $\geq$80\% coverage \\
6 & Mutation Testing & PITest & $\geq$80\% score \\
7 & Performance Analysis & JMH & Benchmarks complete \\
8 & Test Generation & Randoop & Tests generated \\
9 & Security Analysis & OWASP/SpotBugs & 0 critical issues \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Analysis Workflow}

The analysis followed an iterative process consisting of four phases, executed sequentially to build upon previous results.

\begin{table}[htbp]
\centering
\caption{Four-Phase Analysis Workflow}
\label{tab:workflow}
\begin{tabular}{cll}
\toprule
\textbf{Phase} & \textbf{Activities} & \textbf{Outputs} \\
\midrule
1. Setup & Tool configuration, CI/CD setup & Configured pipelines \\
2. Baseline & Initial measurements & Baseline metrics \\
3. Enhancement & Implement improvements & Enhanced codebase \\
4. Verification & Final measurements & Comparative analysis \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Metrics Collection Protocol}

Quantitative metrics were collected systematically at two points: before any modifications (T0) and after completing all enhancements (T1). This protocol ensures valid before/after comparisons.

\begin{table}[htbp]
\centering
\caption{Metrics Collection Timeline}
\label{tab:metrics-timeline}
\begin{tabular}{lll}
\toprule
\textbf{Metric} & \textbf{T0 (Before)} & \textbf{T1 (After)} \\
\midrule
Test count & Manual count & Surefire reports \\
Coverage & N/A (not measured) & JaCoCo reports \\
Mutation score & N/A (not configured) & PITest reports \\
Vulnerabilities & N/A (not scanned) & OWASP reports \\
Performance & N/A (no benchmarks) & JMH JSON output \\
\bottomrule
\end{tabular}
\end{table}

\section{Results: Before and After Comparison}
\label{sec:results}

This section presents a comprehensive comparison between the original Spring PetClinic project and our enhanced version, demonstrating the measurable improvements achieved through systematic dependability analysis. Each criterion is evaluated with quantitative metrics showing the transformation from baseline to final state.

\subsection{Executive Summary}

Table~\ref{tab:executive-summary} provides a high-level overview of the transformation achieved across all nine evaluation criteria. The comparison demonstrates substantial improvements in every measured dimension, with several metrics transitioning from unmeasured or missing states to industry-leading values.

\begin{table}[htbp]
\centering
\caption{Executive Summary: Before vs After Comparison}
\label{tab:executive-summary}
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Before} & \textbf{After} & \textbf{Improvement} \\
\midrule
CI/CD Workflows & 1 & 4 & +300\% \\
Code Coverage & Not measured & 91.9\% & New capability \\
Mutation Score & Not measured & 85\% & New capability \\
SonarCloud Rating & Not integrated & AAA & New integration \\
Security Vulnerabilities & Unknown & 0 Critical/High & Verified secure \\
Docker Health Check & None & Implemented & New feature \\
Performance Benchmarks & None & 8 benchmarks & New capability \\
Total Test Cases & 39 & 556 & +1326\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Criterion 1: CI/CD Pipeline}

The continuous integration infrastructure was significantly expanded from a single basic workflow to a comprehensive multi-workflow system.

\begin{table}[htbp]
\centering
\caption{CI/CD Pipeline: Before vs After}
\label{tab:cicd-before-after}
\begin{tabular}{lll}
\toprule
\textbf{Aspect} & \textbf{Before} & \textbf{After} \\
\midrule
Number of Workflows & 1 & 4 \\
Workflow Names & build.yml only & ci.yml, docker.yml, sonarcloud.yml, latex-pdf.yml \\
Automated Testing & Basic & Full test suite + coverage + mutation \\
Docker Automation & None & Build, tag, push to DockerHub \\
Static Analysis & None & SonarCloud integration \\
Documentation Build & None & Automated LaTeX PDF generation \\
Build Status & Manual verification & Badge integration \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Criterion 2: SonarCloud Integration}

Static code analysis was introduced through SonarCloud integration, achieving top ratings across all quality dimensions.

\begin{table}[htbp]
\centering
\caption{SonarCloud Analysis: Before vs After}
\label{tab:sonar-before-after}
\begin{tabular}{lll}
\toprule
\textbf{Quality Gate} & \textbf{Before} & \textbf{After} \\
\midrule
Integration Status & Not integrated & Fully integrated \\
Reliability Rating & N/A & A \\
Security Rating & N/A & A \\
Maintainability Rating & N/A & A \\
Coverage Tracking & N/A & 91.9\% tracked \\
Code Smells & Unknown & 48 (minor) \\
Technical Debt & Unknown & 2h 30min \\
Quality Gate Status & N/A & Passed \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Criterion 3-4: Docker Image and Container Health}

The Docker infrastructure was enhanced with production-ready features including health monitoring and optimized image configuration.

\begin{table}[htbp]
\centering
\caption{Docker Configuration: Before vs After}
\label{tab:docker-before-after}
\begin{tabular}{lll}
\toprule
\textbf{Feature} & \textbf{Before} & \textbf{After} \\
\midrule
Dockerfile & Basic & Optimized multi-stage \\
Health Check & None & curl localhost:8080/actuator/health \\
Health Check Interval & N/A & 30 seconds \\
Health Check Timeout & N/A & 10 seconds \\
DockerHub Image & None & wario03/petclinic-dependability \\
Image Pulls & N/A & 144+ pulls \\
CI/CD Integration & None & Automated build/push \\
Container Startup & Unchecked & Health verified in CI \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Criterion 5: Code Coverage Analysis}

Code coverage measurement was introduced using JaCoCo, revealing and subsequently improving the test coverage across all packages.

\begin{table}[htbp]
\centering
\caption{Code Coverage: Before vs After}
\label{tab:coverage-before-after}
\begin{tabular}{lll}
\toprule
\textbf{Metric} & \textbf{Before} & \textbf{After} \\
\midrule
Coverage Tool & Not configured & JaCoCo 0.8.12 \\
Instruction Coverage & Unknown & 90\% (8,292/9,213) \\
Branch Coverage & Unknown & 84\% (333/396) \\
Line Coverage & Unknown & 91.9\% \\
Method Coverage & Unknown & 85\% (539/634) \\
Class Coverage & Unknown & 91\% (60/66) \\
Report Generation & None & HTML, XML, CSV \\
CI Integration & None & Automatic per build \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:coverage-by-package} shows the coverage breakdown by package, identifying areas of strength and opportunities for improvement.

\begin{table}[htbp]
\centering
\caption{Coverage by Package}
\label{tab:coverage-by-package}
\begin{tabular}{lcc}
\toprule
\textbf{Package} & \textbf{Instruction Coverage} & \textbf{Branch Coverage} \\
\midrule
model & 98\% & 92\% \\
owner & 93\% & 88\% \\
vet & 91\% & 85\% \\
visit & 89\% & 82\% \\
system & 86\% & 80\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Criterion 6: Mutation Testing}

Mutation testing was implemented using PITest to evaluate test suite effectiveness in detecting code modifications.

\begin{table}[htbp]
\centering
\caption{Mutation Testing: Before vs After}
\label{tab:mutation-before-after}
\begin{tabular}{lll}
\toprule
\textbf{Metric} & \textbf{Before} & \textbf{After} \\
\midrule
Mutation Framework & Not configured & PITest 1.15.0 \\
Total Mutants Generated & N/A & 1,003 \\
Mutants Killed & N/A & 853 \\
Mutants Survived & N/A & 136 \\
Mutation Score & N/A & 85\% \\
Test Strength & N/A & 94\% \\
Mutation Operators & N/A & 7 types active \\
Report Generation & None & HTML, XML \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:mutation-by-type} details the effectiveness against different mutation operator categories.

\begin{table}[htbp]
\centering
\caption{Mutation Kill Rate by Operator Type}
\label{tab:mutation-by-type}
\begin{tabular}{lccc}
\toprule
\textbf{Operator} & \textbf{Generated} & \textbf{Killed} & \textbf{Kill Rate} \\
\midrule
Conditional Boundary & 143 & 128 & 90\% \\
Negate Conditionals & 187 & 165 & 88\% \\
Math Operators & 89 & 78 & 88\% \\
Increment/Decrement & 76 & 67 & 88\% \\
Return Values & 234 & 199 & 85\% \\
Void Method Calls & 198 & 154 & 78\% \\
Empty Returns & 76 & 62 & 82\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Criterion 7: Performance Benchmarking}

Performance analysis was introduced using the Java Microbenchmark Harness (JMH) framework to establish baseline performance metrics.

\begin{table}[htbp]
\centering
\caption{Performance Analysis: Before vs After}
\label{tab:jmh-before-after}
\begin{tabular}{lll}
\toprule
\textbf{Aspect} & \textbf{Before} & \textbf{After} \\
\midrule
Benchmark Framework & None & JMH 1.37 \\
Number of Benchmarks & 0 & 8 \\
Warmup Iterations & N/A & 3 per benchmark \\
Measurement Iterations & N/A & 5 per benchmark \\
Performance Baseline & None & Established \\
Report Format & N/A & JSON (jmh-results.json) \\
CI Integration & N/A & Automated execution \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:jmh-results} presents the benchmark results for core operations.

\begin{table}[htbp]
\centering
\caption{JMH Benchmark Results}
\label{tab:jmh-results}
\begin{tabular}{lrrr}
\toprule
\textbf{Benchmark} & \textbf{Score (ms/op)} & \textbf{Error} & \textbf{Status} \\
\midrule
OwnerCreation & 0.00012 & $\pm$0.00001 & Optimal \\
PetCreation & 0.00008 & $\pm$0.00001 & Optimal \\
VisitCreation & 0.00015 & $\pm$0.00002 & Optimal \\
VetLookup & 0.00023 & $\pm$0.00003 & Optimal \\
OwnerSearch & 0.00089 & $\pm$0.00005 & Optimal \\
PetTypeValidation & 0.00005 & $\pm$0.00001 & Optimal \\
CollectionOperations & 0.00034 & $\pm$0.00004 & Optimal \\
DateProcessing & 0.00018 & $\pm$0.00002 & Optimal \\
\bottomrule
\end{tabular}
\end{table}

All benchmarks completed in sub-millisecond time, indicating excellent performance characteristics for the core domain operations.

\subsection{Criterion 8: Automated Test Generation}

Automated test generation was implemented using Randoop to supplement the existing test suite with automatically generated test cases.

\begin{table}[htbp]
\centering
\caption{Test Generation: Before vs After}
\label{tab:randoop-before-after}
\begin{tabular}{lll}
\toprule
\textbf{Metric} & \textbf{Before} & \textbf{After} \\
\midrule
Test Generation Tool & None & Randoop 4.3.2 \\
Original Test Cases & 39 & 39 (preserved) \\
Generated Test Cases & 0 & 517 \\
Total Test Cases & 39 & 556 \\
Test Increase & N/A & +1326\% \\
Generated Lines of Code & 0 & 28,707 \\
Regression Tests & N/A & Automated \\
Error-Revealing Tests & N/A & Generated \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Criterion 9: Security Analysis}

Comprehensive security analysis was implemented using OWASP Dependency-Check and SpotBugs with the FindSecBugs plugin.

\begin{table}[htbp]
\centering
\caption{Security Analysis: Before vs After}
\label{tab:security-before-after}
\begin{tabular}{lll}
\toprule
\textbf{Aspect} & \textbf{Before} & \textbf{After} \\
\midrule
Dependency Scanning & None & OWASP DC 11.1.1 \\
Static Security Analysis & None & SpotBugs + FindSecBugs \\
Critical Vulnerabilities & Unknown & 0 \\
High Vulnerabilities & Unknown & 0 \\
Medium Vulnerabilities & Unknown & 3 (dependencies) \\
Low Vulnerabilities & Unknown & 5 (informational) \\
Security Report & None & HTML, JSON reports \\
CI Integration & None & Fail on Critical/High \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Overall Impact Assessment}

Table~\ref{tab:overall-impact} summarizes the overall improvement achieved across all dependability dimensions.

\begin{table}[htbp]
\centering
\caption{Overall Dependability Impact Assessment}
\label{tab:overall-impact}
\begin{tabular}{lccc}
\toprule
\textbf{Dimension} & \textbf{Before} & \textbf{After} & \textbf{Assessment} \\
\midrule
Testability & Low & High & Significantly improved \\
Reliability & Unknown & Measured (91.9\%) & Quantified \\
Maintainability & Unknown & AAA Rating & Industry standard \\
Security & Unknown & Verified (0 Critical) & Secured \\
Performance & Unknown & Baselined & Measurable \\
Automation & Minimal & Comprehensive & Fully automated \\
Documentation & Basic & Complete & Academic ready \\
\bottomrule
\end{tabular}
\end{table}

The analysis demonstrates that all nine criteria were successfully met, with quantitative improvements documented for each dimension. The transformation from an unmeasured baseline to a fully instrumented, analyzed, and documented state represents a significant enhancement in overall project dependability.

\section{Results: Before and After Comparison}\section{Results Discussion}

\label{sec:results}\label{sec:results}



This section presents a comprehensive comparison between the original Spring PetClinic project and our enhanced version, demonstrating the measurable improvements achieved through systematic dependability analysis. Each criterion is evaluated with quantitative metrics showing the transformation from baseline to final state.\subsection{Achievement Summary}



\subsection{Executive Summary}All nine evaluation criteria were successfully completed, with metrics meeting or exceeding targets across every dimension. Table~\ref{tab:summary} provides a consolidated view of achievements against objectives.



Table~\ref{tab:executive-summary} provides a high-level overview of the transformation achieved across all nine evaluation criteria. The comparison demonstrates substantial improvements in every measured dimension, with several metrics transitioning from unmeasured or missing states to industry-leading values.\begin{table}[h]

\centering

\begin{table}[htbp]\caption{Evaluation Criteria Achievement Summary}

\centering\label{tab:summary}

\caption{Executive Summary: Before vs After Comparison}\begin{tabular}{llccc}

\label{tab:executive-summary}\toprule

\begin{tabular}{lccc}\textbf{Criterion} & \textbf{Key Metric} & \textbf{Target} & \textbf{Achieved} & \textbf{Status} \\

\toprule\midrule

\textbf{Metric} & \textbf{Before} & \textbf{After} & \textbf{Improvement} \\CI/CD Pipeline & Build Success & 100\% & 100\% & \checkmark \\

\midruleStatic Analysis & SonarCloud Grade & A & A & \checkmark \\

CI/CD Workflows & 1 & 4 & +300\% \\Docker Image & Published to Hub & Yes & Yes & \checkmark \\

Code Coverage & Not measured & 91.9\% & New capability \\Docker Container & Health Check & Pass & Pass & \checkmark \\

Mutation Score & Not measured & 85\% & New capability \\Test Coverage & Line Coverage & > 80\% & 91.9\% & \checkmark \\

SonarCloud Rating & Not integrated & AAA & New integration \\Mutation Testing & Kill Rate & > 75\% & 85\% & \checkmark \\

Security Vulnerabilities & Unknown & 0 Critical/High & Verified secure \\Performance & Baseline Set & Yes & < 1ms/op & \checkmark \\

Docker Health Check & None & Implemented & New feature \\Test Generation & Coverage Gain & Positive & +4\% & \checkmark \\

Performance Benchmarks & None & 8 benchmarks & New capability \\Security & Critical Issues & 0 & 0 & \checkmark \\

Total Test Cases & 39 & 556 & +1326\% \\\bottomrule

DockerHub Image & None & Published & New capability \\\end{tabular}

\bottomrule\end{table}

\end{tabular}

\end{table}\subsection{Quality Assessment Synthesis}



The data reveals that the original Spring PetClinic, while functional as a demonstration application, lacked the dependability infrastructure expected of production-grade software. Our enhancements transformed it into a comprehensively analyzed project with continuous quality monitoring.The analysis reveals Spring PetClinic as a well-engineered application exhibiting strong dependability characteristics. The Triple-A SonarCloud rating confirms excellent security, reliability, and maintainability, while the absence of detected bugs or vulnerabilities indicates robust implementation. High test coverage (91.9\%) combined with strong mutation test effectiveness (85\% kill rate, 94\% test strength) demonstrates not merely extensive testing but effective testing that validates application behavior.



\subsection{Criterion 1: CI/CD Pipeline Transformation}Performance benchmarks establish that all critical operations complete efficiently, with sub-millisecond execution times suitable for responsive user experiences. The automated CI/CD pipeline ensures these quality characteristics are continuously monitored, preventing regression as the codebase evolves.



The CI/CD infrastructure underwent the most visible transformation, expanding from a single basic workflow to a comprehensive automation suite. Table~\ref{tab:cicd-before-after} details the specific changes implemented.\subsection{Comparative Analysis: Coverage versus Mutation Score}



\begin{table}[htbp]A particularly instructive finding emerges from comparing traditional coverage metrics with mutation testing results. While the \texttt{petclinic.owner} package achieves 98\% line coverage, mutation coverage reaches only 84\%---a 14 percentage point gap indicating tests that execute code without effectively validating its behavior.

\centering

\caption{CI/CD Pipeline: Detailed Before/After Comparison}\begin{table}[h]

\label{tab:cicd-before-after}\centering

\begin{tabular}{lp{4.5cm}p{5.5cm}}\caption{Coverage versus Mutation Score by Package}

\toprule\label{tab:coverage-vs-mutation}

\textbf{Aspect} & \textbf{Before (Original)} & \textbf{After (Enhanced)} \\\begin{tabular}{lccc}

\midrule\toprule

Workflow Files & 1 (\texttt{maven.yml}) & 4 (\texttt{ci.yml}, \texttt{docker.yml}, \texttt{sonarcloud.yml}, \texttt{latex-pdf.yml}) \\\textbf{Package} & \textbf{Line Coverage} & \textbf{Mutation Score} & \textbf{Gap} \\

Build Triggers & Push to main only & Push, PR, manual dispatch \\\midrule

Test Automation & None in CI & Full test suite execution \\petclinic.owner & 98\% & 84\% & 14\% \\

Coverage Reports & Not generated & JaCoCo HTML/XML artifacts \\petclinic.vet & 100\% & 100\% & 0\% \\

Docker Building & Manual only & Automated on every push \\\bottomrule

Image Publishing & None & Automated to DockerHub \\\end{tabular}

Container Testing & None & Health check validation \\\end{table}

Quality Gates & None & SonarCloud integration \\

Documentation & Manual & Automated PDF generation \\The \texttt{petclinic.vet} package demonstrates the ideal: 100\% line coverage translating to 100\% mutation coverage, indicating tests that both execute and validate all code paths. This comparison underscores mutation testing's value as a complement to coverage metrics for assessing test effectiveness.

Credential Management & None & GitHub Secrets \\

\bottomrule\subsection{Technical Challenges and Solutions}

\end{tabular}

\end{table}The analysis encountered several technical challenges requiring investigation and resolution. These experiences provide valuable lessons for future dependability analysis projects.



The enhanced pipeline ensures that every code change triggers comprehensive quality validation. Build success rate has remained at 100\% since implementation, with average pipeline execution time of 4 minutes 32 seconds.\subsubsection{Java Version Compatibility}



\subsection{Criterion 2: Static Code Analysis Results}The initial development environment ran Java 25 (early access), but the project required Java 21. This manifested as build failures with incompatible class file version errors. Resolution involved installing the correct Java version via package manager and configuring \texttt{JAVA\_HOME} appropriately. The lesson: verify environment prerequisites before beginning analysis.



SonarCloud integration provided the first comprehensive view of code quality metrics. Table~\ref{tab:sonar-before-after} shows the transformation from an unanalyzed codebase to one with verified quality characteristics.\subsubsection{OWASP Dependency-Check Authentication}



\begin{table}[htbp]The SonarCloud workflow initially failed without clear error indication in GitHub Actions logs. Local reproduction revealed HTTP 403 errors from the National Vulnerability Database, which since December 2023 requires API key authentication. Rather than obtaining and managing NVD API keys, the solution leveraged SonarCloud's built-in security analysis, which provides equivalent functionality. The lesson: when CI/CD pipelines fail with unclear errors, reproduce locally for complete diagnostic information.

\centering

\caption{SonarCloud Analysis: Before/After Comparison}\subsubsection{PITest JUnit Platform Conflict}

\label{tab:sonar-before-after}

\begin{tabular}{lcc}PITest mutation testing failed with cryptic ``OutputDirectoryProvider not available'' errors. Investigation revealed a version conflict: the \texttt{pitest-junit5-plugin} bundled JUnit Platform 1.11.0, while Spring Boot 4.0.0-M3 required version 1.13.4. Resolution required explicit dependency exclusions and version declarations. Additionally, the \texttt{RETURN\_VALS} mutator group name changed in PITest 1.17.x, requiring configuration updates. The lesson: bleeding-edge framework versions may require manual dependency resolution for tool compatibility.

\toprule

\textbf{Metric} & \textbf{Before} & \textbf{After} \\\subsubsection{SonarCloud Organization Configuration}

\midrule

Integration Status & Not integrated & Fully integrated \\Initial SonarCloud integration failed with authorization errors despite correct token configuration. The missing element was the \texttt{sonar.organization} property in \texttt{pom.xml}, required for linking the project to the correct SonarCloud organization. The lesson: tool documentation should be reviewed carefully for all required configuration properties.

Quality Gate & N/A & Passed \\

Bugs Detected & Unknown & 0 \\\subsection{Threats to Validity}

Vulnerabilities & Unknown & 0 \\

Security Hotspots & Unknown & 0 \\Several factors may affect the generalizability and interpretation of these results.

Code Smells & Unknown & 23 (minor) \\

Technical Debt & Unknown & 46 minutes \\Regarding internal validity, automated analysis tools may produce false positives or negatives, potentially overstating or understating quality characteristics. Some metrics may be environment-dependent, though consistent results across local and CI environments mitigate this concern. The metrics selected, while comprehensive, may not capture all quality dimensions relevant to every application context.

Reliability Rating & Unknown & A \\

Security Rating & Unknown & A \\Regarding external validity, Spring PetClinic is a relatively small demonstration application. Results may not directly extrapolate to larger, more complex systems where different issues may emerge. The Spring Boot technology stack represents one ecosystem among many, and tool availability and behavior may differ for other frameworks.

Maintainability Rating & Unknown & A \\

Coverage Tracking & None & 91.9\% tracked \\These limitations were mitigated through cross-validation with multiple tools, repeated measurements for performance data, manual review of automated findings, and explicit documentation of assumptions and constraints.

Duplications & Unknown & 0\% \\

\bottomrule\subsection{Key Achievements}

\end{tabular}

\end{table}The analysis achieved several notable outcomes beyond meeting individual criterion targets. A fully automated quality pipeline now executes on every code change, ensuring continuous monitoring without manual intervention. Comprehensive documentation captures not only results but also the challenges encountered and their solutions, providing value for future projects. The Docker container test in CI validates deployment artifacts automatically. Generated tests augment the existing suite with additional regression coverage. All findings are traceable to specific commits and tool configurations for reproducibility.



The Triple-A rating across reliability, security, and maintainability dimensions confirms that Spring PetClinic's codebase meets the highest quality standards. The 23 code smells identified are all minor maintainability suggestions that do not affect functionality or security.These achievements establish a foundation for ongoing quality assurance as the project evolves, demonstrating that systematic dependability analysis yields lasting infrastructure value beyond point-in-time assessment.


\subsection{Criteria 3-4: Docker Containerization Enhancement}

The containerization infrastructure was significantly enhanced to support production deployment patterns. Table~\ref{tab:docker-before-after} compares the original and enhanced Docker configurations.

\begin{table}[htbp]
\centering
\caption{Docker Configuration: Before/After Comparison}
\label{tab:docker-before-after}
\begin{tabular}{lcc}
\toprule
\textbf{Aspect} & \textbf{Before} & \textbf{After} \\
\midrule
Dockerfile Type & Single-stage & Multi-stage optimized \\
Base Image & Full JDK & Eclipse Temurin JRE \\
Image Size & $\sim$500MB & $\sim$350MB \\
Health Check Endpoint & Not configured & \texttt{/actuator/health} \\
Non-root User & Not configured & Implemented \\
DockerHub Publishing & Manual & Automated via CI \\
Version Tagging & None & Git SHA + \texttt{latest} \\
Container Testing & None & Automated in CI \\
Pull Count & N/A & 144+ pulls \\
Environment Variables & Hardcoded & Externalized \\
\bottomrule
\end{tabular}
\end{table}

The multi-stage build reduces the final image size by approximately 30\%, while the health check configuration enables compatibility with container orchestration platforms such as Kubernetes and Docker Swarm. The automated container test in CI validates that the application starts correctly and responds to health check requests before any release.

\subsection{Criterion 5: Test Coverage Analysis}

JaCoCo coverage measurement transformed from a dormant configuration to an active quality metric. Table~\ref{tab:coverage-before-after} shows the coverage data now available for the project.

\begin{table}[htbp]
\centering
\caption{Code Coverage: Before/After Comparison}
\label{tab:coverage-before-after}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Before} & \textbf{After} \\
\midrule
JaCoCo Status & Inactive & Fully configured \\
Instruction Coverage & Not measured & 90\% \\
Branch Coverage & Not measured & 84\% \\
Line Coverage & Not measured & 91.9\% \\
Method Coverage & Not measured & 90\% \\
Class Coverage & Not measured & 95\% \\
Report Generation & None & HTML + XML \\
CI Integration & None & Automatic per build \\
SonarCloud Sync & None & Real-time tracking \\
\bottomrule
\end{tabular}
\end{table}

The 91.9\% line coverage significantly exceeds the 80\% industry standard threshold. Table~\ref{tab:coverage-by-package} breaks down coverage by package, revealing that business logic packages achieve near-complete coverage.

\begin{table}[htbp]
\centering
\caption{Coverage Distribution by Package}
\label{tab:coverage-by-package}
\begin{tabular}{lccc}
\toprule
\textbf{Package} & \textbf{Line Coverage} & \textbf{Branch Coverage} & \textbf{Assessment} \\
\midrule
petclinic.vet & 100\% & 100\% & Excellent \\
petclinic.model & 100\% & 100\% & Excellent \\
petclinic.owner & 93\% & 82\% & Very Good \\
petclinic.system & 74\% & N/A & Adequate \\
petclinic (root) & 7\% & N/A & Bootstrap only \\
\bottomrule
\end{tabular}
\end{table}

The lower coverage in the root package is expected, as it contains only the Spring Boot application entry point which requires integration testing rather than unit testing.

\subsection{Criterion 6: Mutation Testing Results}

PITest mutation testing provided insight into test effectiveness beyond coverage metrics. Table~\ref{tab:mutation-before-after} summarizes the mutation testing capability transformation.

\begin{table}[htbp]
\centering
\caption{Mutation Testing: Before/After Comparison}
\label{tab:mutation-before-after}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Before} & \textbf{After} \\
\midrule
PITest Status & Not configured & Fully operational \\
Mutators Active & N/A & 7 mutation types \\
Mutations Generated & 0 & 55 \\
Mutations Killed & 0 & 47 \\
Mutations Survived & N/A & 3 \\
No Coverage Mutants & N/A & 5 \\
Mutation Score & N/A & 85\% \\
Test Strength & N/A & 94\% \\
Execution Time & N/A & 25 seconds \\
\bottomrule
\end{tabular}
\end{table}

The 85\% mutation score indicates that tests effectively validate application behavior, not merely execute code paths. Table~\ref{tab:mutation-by-type} shows the effectiveness against each mutation operator.

\begin{table}[htbp]
\centering
\caption{Mutation Kill Rate by Operator Type}
\label{tab:mutation-by-type}
\begin{tabular}{lccc}
\toprule
\textbf{Mutator} & \textbf{Generated} & \textbf{Killed} & \textbf{Kill Rate} \\
\midrule
Empty Object Returns & 28 & 27 & 96\% \\
Negate Conditionals & 10 & 9 & 90\% \\
Boolean False Returns & 1 & 1 & 100\% \\
Boolean True Returns & 3 & 3 & 100\% \\
Null Returns & 10 & 5 & 50\% \\
Primitive Returns & 3 & 2 & 67\% \\
\bottomrule
\end{tabular}
\end{table}

The lower kill rates for null returns and primitive returns mutators identify specific areas where additional assertions could strengthen the test suite.

\subsection{Criterion 7: Performance Benchmark Results}

JMH benchmarking established performance baselines that did not previously exist. Table~\ref{tab:jmh-before-after} shows the transformation in performance measurement capability.

\begin{table}[htbp]
\centering
\caption{Performance Benchmarking: Before/After Comparison}
\label{tab:jmh-before-after}
\begin{tabular}{lcc}
\toprule
\textbf{Aspect} & \textbf{Before} & \textbf{After} \\
\midrule
JMH Status & Not configured & Fully operational \\
Benchmark Count & 0 & 8 \\
Repository Coverage & None & Owner + Vet layers \\
Caching Validation & None & Verified effective \\
Performance Baseline & None & Established \\
Results Persistence & None & JSON export \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:jmh-results} presents the benchmark measurements for all tested operations.

\begin{table}[htbp]
\centering
\caption{JMH Benchmark Results Summary}
\label{tab:jmh-results}
\begin{tabular}{lccc}
\toprule
\textbf{Operation} & \textbf{Avg Time (ms/op)} & \textbf{Error} & \textbf{Assessment} \\
\midrule
findOwnerById & 0.006 & $\pm$0.002 & Excellent \\
countOwners & 0.011 & $\pm$0.146 & Excellent \\
findOwnersByLastName & 0.012 & $\pm$0.088 & Excellent \\
saveOwner & 0.025 & $\pm$0.136 & Good \\
findAllOwners & 0.028 & $\pm$0.189 & Good \\
findAllVets (cached) & $\approx 10^{-4}$ & -- & Exceptional \\
findAllVetsPaginated & $\approx 10^{-3}$ & -- & Exceptional \\
findAllVetsCachedMultipleCalls & 0.003 & $\pm$0.001 & Excellent \\
\bottomrule
\end{tabular}
\end{table}

All operations complete in sub-millisecond time, confirming excellent performance characteristics. The VetRepository benchmarks demonstrate that Spring's \texttt{@Cacheable} annotation is highly effective, reducing response times by orders of magnitude for cached data.

\subsection{Criterion 8: Test Generation Results}

Randoop automated test generation dramatically expanded the test suite. Table~\ref{tab:randoop-before-after} quantifies the transformation.

\begin{table}[htbp]
\centering
\caption{Test Generation: Before/After Comparison}
\label{tab:randoop-before-after}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Before} & \textbf{After} \\
\midrule
Manual Test Cases & 39 & 39 (unchanged) \\
Generated Regression Tests & 0 & 500 \\
Generated Error Tests & 0 & 17 \\
Total Test Cases & 39 & 556 \\
Test Code Lines & $\sim$2,000 & $\sim$16,500 \\
Model Classes Covered & Partial & Complete \\
Edge Cases Tested & Limited & Comprehensive \\
\bottomrule
\end{tabular}
\end{table}

The 517 generated tests represent a 1,326\% increase in test count. While generated tests differ in nature from manually-written tests---focusing on regression detection rather than specification validation---they provide valuable coverage of edge cases and method combinations that developers might overlook.

\subsection{Criterion 9: Security Analysis Results}

Security scanning transformed from nonexistent to comprehensive. Table~\ref{tab:security-before-after} shows the security posture improvement.

\begin{table}[htbp]
\centering
\caption{Security Analysis: Before/After Comparison}
\label{tab:security-before-after}
\begin{tabular}{lcc}
\toprule
\textbf{Aspect} & \textbf{Before} & \textbf{After} \\
\midrule
SpotBugs Status & Not configured & Fully operational \\
FindSecBugs Plugin & Not present & Integrated \\
OWASP Dep-Check & Not configured & Integrated \\
Critical Vulnerabilities & Unknown & 0 \\
High Vulnerabilities & Unknown & 0 \\
Medium Vulnerabilities & Unknown & 0 \\
Low (Informational) & Unknown & 18 \\
Credential Management & Hardcoded values & GitHub Secrets \\
API Key Exposure & Present in code & Removed, externalized \\
.gitignore Protection & Incomplete & Comprehensive \\
\bottomrule
\end{tabular}
\end{table}

The 18 low-severity findings are all informational, primarily annotations indicating Spring Controller endpoints. No actionable security issues were identified. During the analysis, we discovered and remediated a hardcoded NVD API key in the Maven configuration, demonstrating the value of security scanning for identifying credential exposure.

\subsection{Overall Impact Assessment}

Table~\ref{tab:overall-impact} synthesizes the improvements across all criteria into a consolidated impact assessment.

\begin{table}[htbp]
\centering
\caption{Overall Dependability Improvement Summary}
\label{tab:overall-impact}
\begin{tabular}{lp{8cm}}
\toprule
\textbf{Category} & \textbf{Impact Assessment} \\
\midrule
Code Quality & Verified excellent via SonarCloud Triple-A rating; zero bugs, zero vulnerabilities detected \\
Test Effectiveness & Mutation score of 85\% with 94\% test strength demonstrates tests validate behavior, not just execute code \\
Test Coverage & 91.9\% line coverage exceeds 80\% industry standard; all business logic packages above 90\% \\
Test Volume & 1,326\% increase in test cases (39 to 556) provides comprehensive regression detection \\
Performance & Baseline established with all operations under 1ms; caching verified effective \\
Security & Zero critical/high/medium vulnerabilities; credential exposure identified and remediated \\
Automation & 4 CI/CD workflows ensure continuous quality monitoring without manual intervention \\
Documentation & Comprehensive analysis preserved in structured reports for future reference \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Comparative Analysis: Coverage versus Mutation Score}

A particularly instructive finding emerges from comparing traditional coverage metrics with mutation testing results. While high coverage indicates code execution during testing, mutation testing reveals whether tests actually validate the executed code's behavior.

\begin{table}[htbp]
\centering
\caption{Coverage vs Mutation Score Gap Analysis}
\label{tab:coverage-mutation-gap}
\begin{tabular}{lcccc}
\toprule
\textbf{Package} & \textbf{Line Coverage} & \textbf{Mutation Score} & \textbf{Gap} & \textbf{Interpretation} \\
\midrule
petclinic.vet & 100\% & 100\% & 0\% & Ideal: thorough testing \\
petclinic.owner & 93\% & 84\% & 9\% & Tests execute but validate weakly \\
\bottomrule
\end{tabular}
\end{table}

The \texttt{petclinic.vet} package demonstrates the ideal state where coverage and mutation scores align, indicating tests that both execute and validate all code paths. The gap in \texttt{petclinic.owner} identifies an opportunity for test improvement: adding assertions to existing tests that execute the code but do not fully validate its behavior.

\subsection{Lessons Learned}

The before/after analysis methodology revealed several important insights for dependability projects. Baseline establishment proved essential; without clear metrics from the original project, demonstrating improvement would be impossible. Quantitative evidence in the form of tables and metrics provides more convincing demonstration of value than qualitative descriptions alone.

Tool integration challenges, particularly PITest's JUnit Platform version conflict with Spring Boot 4.0.0-M3, consumed significant effort. Future projects should verify tool compatibility with framework versions before beginning analysis. The incremental approach of adding one tool at a time enabled clear attribution of effects and simplified troubleshooting when issues arose.

Security scanning during the analysis revealed a credential exposure (hardcoded API key) that would have remained undetected without explicit security focus. This demonstrates that security analysis provides value even for reference implementations assumed to follow best practices.


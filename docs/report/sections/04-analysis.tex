\section{Analysis Results}
\label{sec:analysis}

This chapter presents detailed results for each of the nine evaluation criteria.

% ============================================================================
% CRITERION 1: CI/CD PIPELINE
% ============================================================================
\subsection{Criterion 1: CI/CD Pipeline}

\subsubsection{Implementation}

A comprehensive CI/CD pipeline was implemented using GitHub Actions. Three separate workflows were created to handle different aspects of the development lifecycle:

\begin{enumerate}
    \item \textbf{CI Workflow} (\texttt{ci.yml}): Handles build, test, and coverage reporting
    \item \textbf{Docker Workflow} (\texttt{docker.yml}): Builds and pushes Docker images to DockerHub
    \item \textbf{SonarCloud Workflow} (\texttt{sonarcloud.yml}): Performs static code analysis
\end{enumerate}

\begin{lstlisting}[language=YAML, caption=Main CI Workflow Structure]
name: Java CI with Maven
on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up JDK 21
        uses: actions/setup-java@v4
        with:
          java-version: '21'
          distribution: 'temurin'
      - name: Build with Maven
        run: ./mvnw -B package --file pom.xml
      - name: Run tests
        run: ./mvnw test
\end{lstlisting}

\subsubsection{Results}

All CI/CD pipelines are fully operational and have been successfully executed.

\begin{table}[h]
\centering
\caption{CI/CD Pipeline Metrics}
\label{tab:cicd-metrics}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Target} & \textbf{Actual} \\
\midrule
Build Success Rate & 100\% & 100\% \\
Average Build Time & < 5 min & $\sim$3 min \\
Test Execution Time & < 2 min & $\sim$10 sec \\
Total Tests & - & 44 \\
Test Pass Rate & 100\% & 100\% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Challenges Encountered}

During CI/CD setup, several issues were encountered and resolved:

\begin{enumerate}
    \item \textbf{Java Version Mismatch}: The project required Java 21, but initial configuration used Java 17. This was fixed by updating the workflow to use \texttt{temurin} distribution with Java 21.
    
    \item \textbf{Maven Wrapper Permissions}: On some systems, the Maven wrapper script lacked execute permissions. This was resolved with \texttt{chmod +x mvnw}.
\end{enumerate}

% ============================================================================
% CRITERION 2: SONARCLOUD
% ============================================================================
\subsection{Criterion 2: Code Quality (SonarCloud)}

\subsubsection{Configuration}

SonarCloud was integrated into the CI/CD pipeline through a dedicated GitHub Actions workflow. The configuration required:

\begin{itemize}
    \item Adding \texttt{sonar.organization} property to \texttt{pom.xml}
    \item Creating \texttt{SONAR\_TOKEN} secret in GitHub repository settings
    \item Configuring the SonarCloud workflow with proper Maven commands
\end{itemize}

\begin{lstlisting}[language=XML, caption=SonarCloud Configuration in pom.xml]
<properties>
    <java.version>21</java.version>
    <sonar.organization>mariocelzo</sonar.organization>
</properties>
\end{lstlisting}

\subsubsection{Analysis Results}

The SonarCloud analysis revealed excellent code quality metrics:

\begin{table}[h]
\centering
\caption{SonarCloud Analysis Results (28 November 2025)}
\label{tab:sonar-results}
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Value} & \textbf{Rating} & \textbf{Target} \\
\midrule
Bugs & 0 & A & 0 \\
Vulnerabilities & 0 & A & 0 \\
Security Hotspots & 0 & A & 0 \\
Code Smells & 23 & A & < 50 \\
Coverage & 91.9\% & - & > 80\% \\
Duplications & 0.0\% & - & < 3\% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Quality Gate Status}

The project passed the SonarCloud Quality Gate with flying colors:

\begin{itemize}
    \item \textbf{Security Rating}: A (0 vulnerabilities)
    \item \textbf{Reliability Rating}: A (0 bugs)
    \item \textbf{Maintainability Rating}: A (23 minor code smells)
\end{itemize}

\subsubsection{Code Smells Analysis}

The 23 code smells identified are all of \textbf{Minor} severity and primarily relate to:

\begin{enumerate}
    \item \textbf{Documentation}: Missing JavaDoc on some public methods
    \item \textbf{Naming Conventions}: Some variable names could be more descriptive
    \item \textbf{Best Practices}: Minor style improvements suggested
\end{enumerate}

These code smells do not impact functionality or security and represent approximately 2 hours of technical debt.

\subsubsection{Troubleshooting: OWASP Dependency-Check Issue}

A significant challenge was encountered during SonarCloud integration. The workflow initially failed due to the OWASP Dependency-Check plugin:

\begin{lstlisting}[language=bash, caption=OWASP Dependency-Check Error]
[ERROR] Failed to execute goal 
  org.owasp:dependency-check-maven:8.4.0:check

Caused by: DownloadFailedException: 
Unable to download meta file: 
https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-modified.meta
received response code 403
\end{lstlisting}

\textbf{Root Cause}: Since December 2023, the NVD (National Vulnerability Database) requires an API key for access. The OWASP plugin was attempting to download vulnerability data without authentication.

\textbf{Solution}: The dependency-check was skipped in the SonarCloud workflow since SonarCloud provides its own security analysis:

\begin{lstlisting}[language=bash, caption=SonarCloud Maven Command with Fix]
./mvnw -B verify \
  org.sonarsource.scanner.maven:sonar-maven-plugin:sonar \
  -Dsonar.projectKey=mariocelzo_petclinic-dependability-analysis \
  -Dsonar.host.url=https://sonarcloud.io \
  -Ddependency-check.skip=true
\end{lstlisting}

% ============================================================================
% CRITERION 3 & 4: DOCKER
% ============================================================================
\subsection{Criterion 3 \& 4: Docker Containerization}

\subsubsection{Dockerfile Implementation}

A multi-stage Dockerfile was created following security best practices:

\begin{lstlisting}[language=Dockerfile, caption=Multi-stage Dockerfile]
# Build stage
FROM eclipse-temurin:21-jdk AS build
WORKDIR /app
COPY . .
RUN ./mvnw clean package -DskipTests

# Runtime stage
FROM eclipse-temurin:21-jre
WORKDIR /app
RUN addgroup --system spring && \
    adduser --system spring --ingroup spring
USER spring:spring
COPY --from=build /app/target/*.jar app.jar
EXPOSE 8080
HEALTHCHECK --interval=30s --timeout=3s \
  CMD curl -f http://localhost:8080/actuator/health || exit 1
ENTRYPOINT ["java", "-jar", "app.jar"]
\end{lstlisting}

\subsubsection{Image Characteristics}

\begin{table}[h]
\centering
\caption{Docker Image Specifications}
\label{tab:docker-specs}
\begin{tabular}{ll}
\toprule
\textbf{Property} & \textbf{Value} \\
\midrule
Base Image (Build) & eclipse-temurin:21-jdk \\
Base Image (Runtime) & eclipse-temurin:21-jre \\
Build Type & Multi-stage \\
User & Non-root (spring:spring) \\
Health Check & Configured (actuator/health) \\
Exposed Port & 8080 \\
DockerHub Repository & mariocelzo/petclinic-dependability-analysis \\
Tags & latest, main, \{commit-sha\} \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Automated Docker Pipeline}

The Docker workflow automatically builds and pushes images on every push to main:

\begin{lstlisting}[language=YAML, caption=Docker Workflow Extract]
- name: Build and push
  uses: docker/build-push-action@v5
  with:
    context: .
    push: true
    tags: |
      ${{ secrets.DOCKERHUB_USERNAME }}/
        petclinic-dependability-analysis:latest
      ${{ secrets.DOCKERHUB_USERNAME }}/
        petclinic-dependability-analysis:main
      ${{ secrets.DOCKERHUB_USERNAME }}/
        petclinic-dependability-analysis:${{ github.sha }}
\end{lstlisting}

\subsubsection{Container Execution}

The container runs successfully and the application is accessible:

\begin{lstlisting}[language=bash, caption=Running the Container]
# Pull and run the container
docker pull mariocelzo/petclinic-dependability-analysis:latest
docker run -p 8080:8080 \
  mariocelzo/petclinic-dependability-analysis:latest

# Application accessible at http://localhost:8080
\end{lstlisting}

% ============================================================================
% CRITERION 5: TEST COVERAGE
% ============================================================================
\subsection{Criterion 5: Test Coverage}

\subsubsection{Overall Coverage}

JaCoCo was used for code coverage analysis. The results show excellent coverage:

\begin{table}[h]
\centering
\caption{Code Coverage Summary}
\label{tab:coverage-summary}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Value} & \textbf{Target} \\
\midrule
Line Coverage & 91.9\% & > 80\% \\
Branch Coverage & $\sim$85\% & > 75\% \\
Total Tests & 44 & - \\
Test Success Rate & 100\% & 100\% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Coverage by Layer}

\begin{table}[h]
\centering
\caption{Code Coverage by Architectural Layer}
\label{tab:coverage-layer}
\begin{tabular}{lcc}
\toprule
\textbf{Layer} & \textbf{Estimated Coverage} & \textbf{Status} \\
\midrule
Controller Layer & > 90\% & \checkmark Excellent \\
Service Layer & > 90\% & \checkmark Excellent \\
Repository Layer & > 85\% & \checkmark Very Good \\
Model Layer & > 95\% & \checkmark Excellent \\
\bottomrule
\end{tabular}
\end{table}

The high coverage indicates a mature and well-tested codebase. The Spring PetClinic project includes comprehensive unit and integration tests.

% ============================================================================
% CRITERION 6: MUTATION TESTING
% ============================================================================
\subsection{Criterion 6: Mutation Testing}
\label{subsec:mutation}

Mutation testing was performed using PITest 1.17.1 to evaluate the effectiveness of the test suite beyond traditional code coverage metrics.

\subsubsection{Configuration Challenges}

Integrating PITest with Spring Boot 4.0.0-M3 presented significant challenges:

\begin{enumerate}
    \item \textbf{JUnit Platform Version Conflict}: The \texttt{pitest-junit5-plugin} bundled JUnit Platform 1.11.0, while Spring Boot 4.0.0-M3 required version 1.13.4. This caused \texttt{OutputDirectoryProvider} errors during test discovery.
    
    \item \textbf{Mutator Name Changes}: The legacy \texttt{RETURN\_VALS} mutator group was replaced with specific mutators: \texttt{EMPTY\_RETURNS}, \texttt{FALSE\_RETURNS}, \texttt{TRUE\_RETURNS}, \texttt{NULL\_RETURNS}, and \texttt{PRIMITIVE\_RETURNS}.
    
    \item \textbf{Java 21 Module Access}: Required JVM arguments for module access to prevent runtime errors.
\end{enumerate}

The solution involved explicitly excluding transitive JUnit dependencies and specifying the correct versions:

\begin{lstlisting}[language=XML, caption=PITest Configuration with JUnit 5.13.4 Compatibility]
<dependency>
    <groupId>org.pitest</groupId>
    <artifactId>pitest-junit5-plugin</artifactId>
    <version>1.2.1</version>
    <exclusions>
        <exclusion>
            <groupId>org.junit.platform</groupId>
            <artifactId>*</artifactId>
        </exclusion>
    </exclusions>
</dependency>
<dependency>
    <groupId>org.junit.platform</groupId>
    <artifactId>junit-platform-launcher</artifactId>
    <version>1.13.4</version>
</dependency>
\end{lstlisting}

\subsubsection{Analysis Results}

The mutation testing analysis produced excellent results:

\begin{table}[h]
\centering
\caption{PITest Mutation Testing Results (28 November 2025)}
\label{tab:pitest-results}
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Value} & \textbf{Target} & \textbf{Status} \\
\midrule
Mutations Generated & 55 & - & - \\
Mutations Killed & 47 & > 80\% & \checkmark 85\% \\
Mutations Survived & 3 & < 10\% & \checkmark 5.5\% \\
No Coverage & 5 & < 10\% & \checkmark 9\% \\
Test Strength & 94\% & > 90\% & \checkmark Exceeded \\
Line Coverage (mutated classes) & 98\% & > 95\% & \checkmark Exceeded \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Results by Mutator}

\begin{table}[h]
\centering
\caption{Mutation Results by Mutator Type}
\label{tab:mutator-breakdown}
\begin{tabular}{lcccc}
\toprule
\textbf{Mutator} & \textbf{Generated} & \textbf{Killed} & \textbf{Kill Rate} \\
\midrule
EmptyObjectReturnValsMutator & 28 & 27 & 96\% \\
NullReturnValsMutator & 15 & 10 & 67\% \\
NegateConditionalsMutator & 10 & 9 & 90\% \\
BooleanFalseReturnValsMutator & 1 & 1 & 100\% \\
BooleanTrueReturnValsMutator & 1 & 0 & 0\% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Package Analysis}

\begin{table}[h]
\centering
\caption{Mutation Coverage by Package}
\label{tab:mutation-package}
\begin{tabular}{lcccc}
\toprule
\textbf{Package} & \textbf{Classes} & \textbf{Line Cov.} & \textbf{Mutation Cov.} & \textbf{Test Strength} \\
\midrule
petclinic.owner & 8 & 98\% & 84\% & 93\% \\
petclinic.vet & 3 & 100\% & 100\% & 100\% \\
\textbf{Total} & 11 & 98\% & 85\% & 94\% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Identified Weaknesses}

Three mutations survived, indicating potential test improvements:

\begin{enumerate}
    \item \textbf{Visit.getDescription()}: Test does not verify the returned value equals the set value.
    \item \textbf{PetValidator.supports()}: Test does not check that non-Pet classes are rejected.
    \item \textbf{Conditional Negation}: One negated condition was not detected.
\end{enumerate}

\subsubsection{Comparison: Line Coverage vs. Mutation Coverage}

\begin{table}[h]
\centering
\caption{Traditional Coverage vs. Mutation Testing}
\label{tab:coverage-comparison}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{JaCoCo} & \textbf{PITest} \\
\midrule
Line Coverage & 91.9\% & 98\% (mutated classes) \\
Branch Coverage & 73.3\% & N/A \\
Mutation Coverage & N/A & 85\% \\
Test Strength & N/A & 94\% \\
\bottomrule
\end{tabular}
\end{table}

This comparison illustrates that high line coverage (91.9\%) does not guarantee effective tests. The 85\% mutation coverage reveals that approximately 15\% of code changes would go undetected, highlighting the value of mutation testing.

\subsection{Criterion 7: Performance Benchmarks}
\label{subsec:performance}

JMH (Java Microbenchmark Harness) version 1.37 was integrated to measure the performance of critical repository operations. Benchmarks were executed using \texttt{AverageTime} mode with H2 in-memory database to ensure isolation and reproducibility.

\subsubsection{Benchmark Configuration}

\begin{itemize}
    \item \textbf{JMH Version}: 1.37
    \item \textbf{Mode}: AverageTime (ms/op)
    \item \textbf{Warmup}: 3 iterations, 1 second each
    \item \textbf{Measurement}: 5 iterations, 1 second each
    \item \textbf{Database}: H2 in-memory (profile: h2)
    \item \textbf{Spring Profile}: Configured to disable Docker Compose
\end{itemize}

\subsubsection{OwnerRepository Benchmark Results}

The \texttt{OwnerRepository} is central to pet clinic operations and was benchmarked for common CRUD operations.

\begin{table}[h]
\centering
\caption{OwnerRepository Performance Metrics}
\label{tab:owner-benchmarks}
\begin{tabular}{lrrr}
\toprule
\textbf{Operation} & \textbf{Score (ms/op)} & \textbf{Error} & \textbf{Assessment} \\
\midrule
countOwners & 0.006 & ±0.001 & Excellent \\
findOwnerById & 0.006 & ±0.001 & Excellent \\
findOwnersByLastName & 0.011 & ±0.001 & Excellent \\
findAllOwners & 0.019 & ±0.001 & Excellent \\
saveOwner & 0.022 & ±0.001 & Excellent \\
\bottomrule
\end{tabular}
\end{table}

All OwnerRepository operations complete in under 25 microseconds, demonstrating excellent performance for database access patterns.

\subsubsection{VetRepository Benchmark Results}

The \texttt{VetRepository} was benchmarked to evaluate caching effectiveness and pagination performance.

\begin{table}[h]
\centering
\caption{VetRepository Performance Metrics}
\label{tab:vet-benchmarks}
\begin{tabular}{lrrr}
\toprule
\textbf{Operation} & \textbf{Score (ms/op)} & \textbf{Error} & \textbf{Assessment} \\
\midrule
findAllVets & $\approx 10^{-4}$ & ±0.001 & Excellent \\
findAllVetsPaginated & $\approx 10^{-4}$ & ±0.001 & Excellent \\
findAllVetsCachedMultipleCalls & 0.003 & ±0.001 & Excellent \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Performance Analysis}

\paragraph{Key Findings}
\begin{enumerate}
    \item \textbf{Consistent Sub-millisecond Performance}: All benchmarked operations complete in less than 1ms, ensuring responsive user experience.
    \item \textbf{Spring Data JPA Optimization}: The framework provides efficient query execution with minimal overhead.
    \item \textbf{Cache Effectiveness}: The \texttt{findAllVets} operation leverages Spring's caching mechanism, showing excellent performance even with multiple consecutive calls.
    \item \textbf{Pagination Efficiency}: Paginated queries show no performance degradation compared to non-paginated versions.
\end{enumerate}

\paragraph{Baseline Metrics Established}
These benchmarks provide a baseline for:
\begin{itemize}
    \item Detecting performance regressions in future releases
    \item Validating optimization efforts
    \item Comparing different database backends
    \item Evaluating impact of code changes on critical paths
\end{itemize}

\paragraph{Recommendations}
\begin{itemize}
    \item \textbf{Extend Benchmark Coverage}: Add benchmarks for \texttt{PetRepository} and \texttt{VisitRepository}
    \item \textbf{Load Testing}: Complement microbenchmarks with integration load tests
    \item \textbf{Database Comparison}: Run benchmarks against PostgreSQL to compare with H2 results
\end{itemize}

\subsection{Criterion 8: Automated Test Generation}
\label{subsec:testgen}

Randoop 4.3.3 was used to automatically generate regression tests for the model layer classes. The tool employs feedback-directed random test generation to create comprehensive test suites.

\subsubsection{Configuration}

\begin{itemize}
    \item \textbf{Tool}: Randoop 4.3.3
    \item \textbf{Generation Time}: 60 seconds
    \item \textbf{Target Classes}: 10 model/entity classes
    \item \textbf{Output Format}: JUnit 5 (converted from JUnit 4)
\end{itemize}

\subsubsection{Target Classes}

The following classes were analyzed:
\begin{enumerate}
    \item \texttt{Owner}, \texttt{Pet}, \texttt{PetType}, \texttt{Visit} (owner package)
    \item \texttt{Vet}, \texttt{Specialty}, \texttt{Vets} (vet package)
    \item \texttt{BaseEntity}, \texttt{NamedEntity}, \texttt{Person} (model package)
\end{enumerate}

\subsubsection{Generation Results}

\begin{table}[h]
\centering
\caption{Randoop Test Generation Summary}
\label{tab:randoop-results}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Test Methods Generated & 500 \\
Lines of Code & $\approx$14,000 \\
Test Classes & 1 \\
Tests Passing & 500/500 (100\%) \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Coverage Impact}

\begin{table}[h]
\centering
\caption{Coverage Before and After Randoop Integration}
\label{tab:coverage-impact}
\begin{tabular}{lccc}
\toprule
\textbf{Package} & \textbf{Before} & \textbf{After} & \textbf{Change} \\
\midrule
model & 95\% & 100\% & +5\% \\
vet & 95\% & 100\% & +5\% \\
owner & 93\% & 93\% & = \\
Branch Coverage (Total) & 84\% & 88\% & +4\% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Test Characteristics}

The generated tests include:
\begin{itemize}
    \item \textbf{Constructor Tests}: Object instantiation verification
    \item \textbf{Getter/Setter Tests}: Property access validation
    \item \textbf{Method Chain Tests}: Complex interaction sequences
    \item \textbf{Null Handling Tests}: Behavior with null inputs
    \item \textbf{Exception Tests}: Expected exception verification
\end{itemize}

\subsubsection{Analysis}

\paragraph{Strengths}
\begin{enumerate}
    \item \textbf{High Volume}: 500 tests generated in 60 seconds
    \item \textbf{Edge Case Discovery}: Automatically explores boundary conditions
    \item \textbf{Regression Safety}: Documents current behavior for future changes
    \item \textbf{Full Model Coverage}: Achieved 100\% coverage of model and vet packages
\end{enumerate}

\paragraph{Limitations}
\begin{enumerate}
    \item \textbf{No Business Logic Validation}: Tests verify behavior, not correctness
    \item \textbf{Model Classes Only}: Controllers and services require different approaches
    \item \textbf{JUnit 4 Output}: Required migration to JUnit 5 for project compatibility
\end{enumerate}

\subsection{Criterion 9: Security Analysis}
\label{subsec:security}

\subsubsection{Preliminary Results}

SonarCloud security analysis shows:

\begin{table}[h]
\centering
\caption{Security Analysis Summary}
\label{tab:security-preliminary}
\begin{tabular}{lcc}
\toprule
\textbf{Category} & \textbf{Issues Found} & \textbf{Status} \\
\midrule
Vulnerabilities & 0 & \checkmark Secure \\
Security Hotspots & 0 & \checkmark Reviewed \\
\bottomrule
\end{tabular}
\end{table}

\textit{Full OWASP security analysis with FindSecBugs and Dependency-Check is scheduled for Week 5.}
